APP_MODE=development
INPUT_DOC_FOLDER=/path/to/your/document/folder
USE_SAMPLE_DOCS="true"

# The model to use for embedding (must be available via Ollama)
# Recommended model
EMBEDDING_MODEL='mxbai-embed-large:v1'
# Alternative models
# EMBEDDING_MODEL='nomic-embed-text'
# EMBEDDING_MODEL='BAAI/bge-large-en-v1.5'
# For Italian
# EMBEDDING_MODEL='BAAI/bge-m3'

# The LLM to use for generation (must be available via Ollama)
LLM=llama3.2
# For italian
# LLM=VitoF/llama-3.1-8b-italian

# Qdrant vector database connection settings
VECTOR_STORE_HOST=localhost
VECTOR_STORE_PORT=6333
VECTOR_STORE_DOC_COLLECTION_NAME=semantic_search_docs
VECTOR_DIMENSION=1024
